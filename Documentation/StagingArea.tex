\chapter{Staging Area}



\subsection{Distance Metrics}
A measure of similarity between data points is a vital component to clustering algorithms.  The suitability of any given measure is dependent on the generative process providing the data.

\subsection{Primer on Spectral Graph Theory}
Modern spectral graph theory increasingly takes insights from geometry.  Discrete analogues of isoperimetry results and heat flow on manifolds are just a few examples being put to use in modern applications.  The normalized graph Laplacian is used to aid in consistency between spectral geometry and stochastic processes.  We consider connected graphs $G = (E,V)$ in this work, in which case we can define the normalized graph Laplacian as $\mathcal{L} = T^{\frac{1}{2}} L T^{\frac{-1}{2}} = I - T^{\frac{1}{2}} A T^{-\frac{1}{2}}$, where $A$ is the adjacency matrix, L is defined by

\begin{equation}
L(u,v)  = \Biggl\{
\begin{array}{cc}
 d_v & :\; u=v \\
 -1  & : u\sim v  \\
 0   & : u \nsim v  \end{array}
\end{equation} and
$T = diag\{d_1, \cdots , d_n\}$ where $d_v$ is the degree of vertex $v$.

$\mathcal{L}$ is a difference operator :

\begin{equation}
 \mathcal{L}  = \frac{1}{\sqrt{d_u}} \sum\limits_{}^{v : u \sim v} ( \frac{g(u)}{\sqrt{d_u}} -  \frac{g(v)}{\sqrt{d_v} } )
\end{equation}

\begin{eqnarray}
Vol (G) = \sum\limits_{v \in V}^{d_v} = Tr(T) \\
\sigma(\mathcal{L}) \in \dblr^+ \\
ker ( \mathcal{L} ) = span\{ T^{\frac{1}{2}} \mathbb{1} \}
\end{eqnarray}

\subsection{Generalized Chebyshev Bounds on Quadratic Sets via Semidefinite Programming}

Boyd et al  \citet{SDPVandenberghe_generalizedchebyshev} provide a simplified development of an algorithm to compute the lower bound on the probability of a set which is defined by quadratic inequalities. That algorithm is discussed here.

\begin{equation}
\min (1-  \sum\limits_{i=1}^{m} \lambda_i)  \ni Tr( A_i z_i) + 2 b_{i}^{T} z_i + c_i \lambda_i \geqslant 0 \;\;\; \forall i=1, ... , m
\end{equation}

\begin{equation}  \sum\limits_{i=1}^{m}  [
\begin{array}{cc}
z_i & z_i \\
z_i & \lambda_i \\
\end{array} ] \succeq 0 \end{equation}

\begin{equation}  C = \{ x \in \dblr :  x^T A_i x + 2 b_{i}^{T} +c_i <0 : i=1, ...,m \} \end{equation}

\begin{equation}
\min E[f_0(X)] \ni E[f_i(X)] = a_i : i=1, ...,m
\end{equation}
 moment constraints

Let
\begin{equation}
 \bar{x} \in \dblr^n S \subset S^n \ni S \succeq \bar{x} \bar{x}^T
\end{equation}
  and define
 \begin{equation}
 P(C,\bar{x},S) = inf_{\mathcal{P}(\dblr^n)} \{P(X \in C) \mid E[X] = \bar{x} E[X X^T] = S \}
 \end{equation}

The optimization problem is to find $ P \in \mathcal{P}(\dblr^n) $ - a probability density function which maximizes the probability of the convex set C and satisfies the moment constraints.


\subsection{Diffusion Map}
\cite{DMBremerabstractdiffusion}, \cite{DMCarnegieinformationdiffusion}, \cite{DMCoifmandiffusionmaps},
\cite{DMKubota00reactiondiffusionsystems}, \cite{DMLafferty05diffusionkernels},
\cite{DMNadler06diffusionmaps}.

Spectral clustering involves constructing a Markov chain over a graph is constructed over the graph of the data and using the sign of the first non-constant eigenvector for graph cuts and cluster localization.  This approach can be generalized to higher-order eigenvectors yielding a multi-resolution view of the data. Using multiple eigenvectors allows one to embed and parameterize the data in a lower dimensional space.  Examples of this procedure include LLE, Laplacian \& Hessian Eigenmaps.  The common theme among these approaches is that eigenvectors of a Markov process can encode coordinates of the data set on a low dimensional manifold in a Euclidian space.  The advantage over conventional methods is that the representation is non-linear and they preserve local structure. Kernel eigenmap embeddings can be generalized into a diffusion  framework where a discrete Laplacian acts on a low dimensional representation space.  This allows for a true multi-scale parametrization.  Iterating a Markov process involves computing power of the transition matrix to run a random walk of the graph forward in time.   By construction a one parameter map defining the diffusion and specifying boundary conditions the full power of diffusions on a smooth manifold may be brought to bear on parameterizing the geometry of the data.  Different boundary conditions and diffusion operators give rise to a discrete approximations of familiar stochastic PDE's.

Let $(X,\mathcal{A},\mu)$ be a measure space and $\quad k: X \times Y \longrightarrow \dblr $ a kernel function.
 \begin{eqnarray}
 d(x) &=& \int\limits_{X} k(x,y) d \mu(y)  \\
 P(x,y) &=& \frac{ k(x,y) }{ d(x) }    \\
 (D_t (x,y))^2 &=& \norm{P_t(x, \cdot) - P_t(y,\cdot )}_{ L^2(X,\frac{d\mu}{\pi}) }  \\
 \pi(\mu) &=&  \frac{d(y)}{z \in Z^{d(z)} } \\
 \pi(x) p(x,y) &=& \pi(y) p(y,x)
 \end{eqnarray}


$D_t (x,y)$ is the functionally weighted $L^2$ distance between the 2 posteriors $\mu \rightarrow P_t(x,u) $ and $\mu \rightarrow P_t(y,u) $.  This is related to isoperimetry. Think about what happens as the cardinality of paths connecting $x$ and $y$ is increased.  $D_t$ can be computed using the eigenvalues of $P$.

\begin{equation}
D_t (x,y) = \sqrt{ \sum\limits_{\lambda \geq 1 }{} \lambda_{l} ( \phi_l (x) - \phi_l (y) )^2   }
\end{equation}

We can define an embedding in Euclidian space via

\begin{equation}
\Psi_t (x) = \{ \lambda_{1}^{t} \phi_1(x) , ...  \lambda_{s(\delta,t)}^{t} \phi_{s(\delta,t)} (x) \}
\end{equation}

\subsection{The Matrix Exponential}
The matrix exponential of a matrix $\mat{A}$ is defined as
\begin{align*}
  e^{\mat{A}}
  &= \mat{I} + \mat{A} + \frac{\mat{A}^2}{2!} + \dots \\
  &= \sum_{k = 0}^\infty \frac{\mat{A}^k}{k!}.
\end{align*}

The Pade approximation to
$e^{\mat{A}}$ is
\begin{displaymath}
  e^{\mat{A}} \approx R(\mat{A}),
\end{displaymath}
with
\begin{align*}
  R_{pq} (\mat{A})
  &= (D_{pq}(\mat{A}))^{-1} N_{pq}(\mat{A}) \\
  \intertext{where}
  D_{pq}(\mat{A})
  &= \sum_{j=1}^p \frac{(p+q-j)! p!}{ (p+q)!j!(p-j)!}\, \mat{A}^j \\
  \intertext{and}
  N_{pq}(\mat{A})
  &= \sum_{j=1}^q \frac{(p+q-j)! q!}{ (p+q)!j!(q-j)!}\, \mat{A}^j.
\end{align*}
See \cite{Moler78nineteendubious} for a detailed accounting of this and other matters regarding the calculation of the matrix exponential.
%\citet{Moler78nineteendubious} \citep*{Moler78nineteendubious} \citep{Moler78nineteendubious} \citet*{Moler78nineteendubious}

\subsection{Spectral Geometry}
Spectral Geometry concerns itself with the relationships between a geometric structure and the spectra of a differential operator, typically the Laplacian.   Inferring the geometry from the spectra is a type of inverse problem since two non isometric manifolds may share the same spectra.  Going the other way, we encounter isoperimetric inequalities and spectral gap theorems.  "Can One Hear the Shape of a Drum?" was the of an article by Mark Kac in the American Mathematical Monthly 1966.   The frequencies at which a drum vibrate depends on its shape. The elliptic PDE  $ \nabla^2 A + k A = 0$ tells us the frequencies if we know the shape. These frequencies are the eigenvalues of the Laplacian in the region.  Can the spectrum of the Laplacian  tell us the shape if we know the frequencies?  Hermann Weyl showed the eigenvalues of the Laplacian in the compact domain $\Omega$ are distributed according to $ N(\lambda) \sim (2 \pi)^{-d) \omega_d \lambda^{\frac{d}{2}} vol(\Omega}$

\subsection{Sparse Representation}
A Gaussian distribution is often an accurate density model for low dimensional data, but very rarely for high-dimensional data. High dimensional data is less likely to be Gaussian, because of the high degree of independence this demands.  Recall the a Gaussian is a rotation of a distribution with completely independent coordinates. In a typical high dimensional application, one may be able to find a few features that are approximately independent, but generally as more features are added the dependencies between them will grow.

Diaconis and Freedman showed that for \textit{most} high dimensional point clouds, \textit{most} low dimensional orthogonal projections are a mixture of normal spherically symmetric distributions.

\begin{lem}[Poincare Lemma]
If $\sigma_n$ is uniform on $\sqrt{n}S_{n-1} \in \dblr^n$,  $d<n$ and
\begin{equation*}
\Pi_{d,n} ( x_1, \hdots , x_n) \rightarrow ( x_1, \hdots , x_n)
\end{equation*}
is the canonical projection, then for fixed $d$, as $ n \rightarrow \infty $, we have that
$\Pi_{d,n}$ converges weakly towards a centered reduced Gaussian distribution on $\dblr^d$
\end{lem}

Proof [See pp55 Some Aspects of Brownian Motion : Some Recent Martingale Problems].
Uee LLN.  If $(X_1,X_2, \hdots ,X_n)$ iid $N(0,1)$, then
\begin{equation*}
\frac{1}{n} \rho_{n}^{2} =: \frac{1}{n} \sum_{i=0}^{n} x_{i}^{2} \rightarrow 1  \rightarrow \infty
\end{equation*}
If we define $\tilde{X}_{(n)} = (X_1,X_2, \hdots ,X_n) = \frac{1}{\sqrt{n}} \rho_n \theta_n$ where $\theta_n \sim \sigma_n$ a uniform distribution on $\sqrt{n}S_{n-1}$.  Then the lemma follows from the equation $\tilde{X}_{(n)} = \frac{1}{\sqrt{n}} \rho_n  \Pi_{d,n} (\theta_n)$.

\subsection{Concentration of Measure}
 \cite{MCArora04expanderflows}, \cite{MCBartlett03convexity}, \cite{MCBoucheron04concentrationinequalities},
 \cite{MCFRIEDMAN96computingbetti}, \cite{MCLedoux04spectralgap}, \cite{MCMuyan_ablessing},
 \cite{MCSinclair92improvedbounds}, \cite{MCTalagrand95concentrationof}.

The Chernoff and Hoeffding bounds tell us that the average of $n$ iid  random variables $X_1,X_2, \hdots ,Xn$ is tightly concentrated around its mean if ${X_i}$ are bounded and $n$ is sufficiently large. hat about $G(X_1,X_2, \hdots ,X_n)$?
The feature of the average which gives rise to tight concentration is that is is Lipschitz. The following concentration bound applies to any Lipschitz function of iid normal random variables. See Ledoux (2001, page 41, 2.35).

High dimensional space is mostly empty.  This is more commonly called the \textit{"curse of dimensionality"}.  One way to get around the curse of dimensionality is to find interesting projections.  Many common algorithms such as principal components, multidimensional scaling, and factor analysis fall into this category.  Huber \cite{HuberProjectionPursuit} placed many of these in to a common framework called projection pursuit.

Logarithmic Sobolev inequalities have a close relationship with the concentration of measure phenomena.  There are two major types of concentration; Gaussian and Exponential. [see Ledoux]

Let $(e^{-At})_{t\geq 0}= (T_t)_{t\geq 0}$ be a symmetric Markov
semigroup on $ L^2(X,d{\mu})$ with generator $A$ defined on   a ${\sigma}$-finite
measure space $(X,d{\mu})$. $(T_t)_{t\geq 0}$ is ultracontractive if
for any $t>0$, there exists a finite positive number $a(t)$ such
that, for all $f\in L^1$ :
\begin{equation}\label{ult1}
\|T_tf\|_{\infty}  \leq a(t) \|f\|_1.
\end{equation}

An equivalent formulation (by interpolation) of ultracontractivity is
that for any $t>0$, there exists a finite positive number  $c(t)$ such
that,  $\forall f\in L^2$,
\begin{equation}\label{ult2}
\|T_tf\|_{\infty} \leq c(t) \|f\|_2
\end{equation}
 Also by duality, the inequality (\ref{ult2}) is equivalent to
\begin{equation}\label{ult3}
\|T_tf\|_{2} \leq c(t) \|f\|_1
\end{equation}
It is known that, under the assumptions on the semigroup
$(T_t)_{t\geq 0}$, (\ref{ult2}) implies (\ref{ult1})
with $a(t)\leq c^2(t/2)$
and
(\ref{ult1}) implies (\ref{ult2})  with $c(t) \leq \sqrt{a(t)}$.
\\

We say that the generator $A$ satisfies  LSIWP  (logarithmic Sobolev inequality
with parameter) if  there exist a monotonically decreasing continuous function
${\beta}: (0,+\infty)\rightarrow (0,+\infty)$ such that
\begin{equation}\label{lsiwp}
\int f^2\log f\, d{\mu} \leq
\epsilon Q(f) +{\beta}(\epsilon) \|f\|^2_2 + \|f\|^2_2\log \|f\|_2
\end{equation}
for all $\epsilon >0$ and $0\leq f\in \mbox{Quad}(A)\cap L^1\cap
L^{\infty}$ where
$\mbox{Quad}(A)$ is the domain of $\sqrt{A}$ in $L^2$ and
$Q(f)=(\sqrt{A}f,\sqrt{A}f)$.
\\

This inequality is modeled on the Gross inequality \cite{}.
\\

In \cite{ds},\cite{d}, the authors show that LSIWP implies
ultracontractivity property  under an integrability condition on
$\beta$. This condition can be enlarged and be stated as follows:

\begin{thm}
Let ${\beta}(\epsilon)$ be a monotonically decreasing continuous
function of $\epsilon$
such that
\begin{equation}\label{vareps}
\int f^2\log f \, d{\mu}\leq
\epsilon Q(f) +{\beta}(\epsilon)\, \|f\|^2_2 + \|f\|^2_2\log \|f\|_2
\end{equation}
for all $\epsilon >0$ and $0\leq f\in \mbox{Quad}(A)\cap L^1\cap
L^{\infty}$. Suppose that
for one ${\eta}>-1$,
\begin{equation}\label{integral}
M_{\eta}(t)=({\eta}+1)t^{-({\eta}+1)})\int_0^t
{s}^{\eta}{\beta}\left(\frac{s}{\eta+1}\right)
\,ds
 \end{equation}
is finite for  all $t>0$. Then $e^{-At}$ is ultracontractive
and
\begin{equation}\label{majo}
\| e^{-At} \|_{\infty,2}\leq e^{M_{\eta}(t)}
\end{equation}
for all $0<t<\infty$.
\end{thm}


\subsection{Primal Dual Theory}
 \cite{Chapelle07traininga}



%\begin{itemize}
%  \item $x_1 = $ 0.99th quantile  of p53 Cell Mean Intensity
%  \item $x_2 = $ 0.99th quantile of HIF1A Cell Mean Intensity
%  \item $x_3 = $ 0.05th quantile of beta catenin Cell Mean Intensity
%  \item $x_4=  $ 0.50th quantile of COX2 Cell Plasma Nuc Ratio
%\end{itemize}
%
%\begin{equation*}
%p_{progression} = \frac{1}{e^{-z} } : z = \beta_0 + x_1 * \beta_1 + x_2 * \beta_2 + x_3 * \beta_3 + x_4 * \beta_4
%\end{equation*}
%
%Where $p_{progression}$ is the probability of progression, and $\beta_i$ are obtained via fitting a generalized linear model using a logit link function.
