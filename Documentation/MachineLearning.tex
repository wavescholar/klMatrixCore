\chapter{MachineLearning}


\section{Kernel Density Estimation}
To define the empirical distribution function of a sample of size $N$ - place mass $1/N$ at each member of the sample.  This forms a nonparametric estimate of the marginal density $P(X)$.  This is a singular form of kernel smoothing for density estimation.  If $\psi$ belongs to some nice class of function, and $\int\limits_{\infty}^{-\infty}\psi(x) dx = 1$, we can form a parametric estimator for the pdf of a process from a sample
population of size $N$ by calculating
\begin{equation} p(x;\theta)=\frac{1}{N \theta}
\sum\limits_{i-1}^{N} \phi( \frac{x-X_n}{\theta})
\end{equation}  If $\phi$ happens to be a density then $p(x,
\theta)$ is also a density.  Letting $\theta \rightarrow 0$ for
the right kernel, we get the empirical density of the sample
%bbcrevisit - provide kernel that makes the limit ok
population.  The mean squared error of the estimator expressed
as a bias term and a variance term is \begin{equation} Err
p(x;\theta)= E[p(x;\theta)-p(x)]^2 =  E[ p(x;\theta)-p(x)]^2 =
(E[p(x;\theta)]-p(x))^2 + Var[ p(x;\theta)]
\end{equation}

\section{Covariance Matrix Estimation } For numerical stability
in regression algorithms, the covariance matrix needs to be
positive definite.  An well conditioned estimator for the
covariance matrix of a process can be obtained by mixing the
sample covariance with the identity matrix.  This is a linear
shrinkage estimator based on a modified Frobenius norm for $A
\in M_{mn}$  \begin{equation} ||\mathbf{A}||_{\cal{F}}= \sqrt{
\frac{tr (A A^t)}{n}} \end{equation}  Without loss of
generality, set $\mu =0$ and let $\widehat{\Sigma} = \alpha
\mathbb{I} + \beta \mathbf{S}$ where $\mbf{S}=\frac{\mbf{X}^T
\mbf{X}}{n}$ is the sample covariance.  We seek to minimize $E(
||\widehat{\Sigma} - \Sigma||^2)$, but since we don't know the
true population covariance matrix, we have to form an
approximation.

\section{Learning Theory and Functional Analysis}
Supervised learning in it's most abstract setting requires
finding a function $f(x)$ given instances ${ (x_i ,f(x_i))}$.
Typical assumptions are that ${x_i}$ is an iid sample from some
unknown distribution. A loss function is a random variable \[ L
: Ran(f) \times Ran(f) \rightarrow \dblr^+\] defining the cost
of misclassification.  The risk associated with a candidate
function $f'$ is defined to be the expectation of the loss over
the sample space $\Omega$, \begin{equation} R(f')=\int L(
f(\omega), f'(\omega)) d\omega\end{equation}.  Statistical
learning theory is concerned with assessing the approximations
to $f$ given by minimizing the empirical loss associated with a
sample ${ (x_i ,f(x_i))}$.

The notion of a loss function goes back to the roots of modern
probability theory and economics. The St. Petersburg paradox is
an example of a random variable $S : \dbln \rightarrow \dblr^=$
with infinite expectation, but where ???. Let $W(k)$ be the
winnings after k plays of from a game with outcome $S$ that
pays $2^{i-1}$ with probability $p_i=1/2^i$. $\lim_{k\To\infty}
W(k)/k =E(S)=\sum\limits_{i=1}^{\infty} p_i 2^{i-1}= \infty$
The implication for a decision theory based only expected value
is that a rational player would pay an infinite amount of money
to play this game. Bernoulli introduced the notion of expected
utility which takes into account the fact that a payout of
$2^i$ may not have twice the utility of a payout of $2^{i+1}$
when $i$ gets large.  The utility $U$ is a random variable on
the sample space representing preferences of an agent.  Loss
represents the aversion of an agent to the outcomes of the
sample space,
\[L(\omega)+U(\omega) = \alpha \fall \omega \in \Omega\]
where $\alpha$ is constant.  Expected loss $R(f')$ is the risk
associated with choosing the approximation $f'$. Restricting
the class of functions to consider when minimizing the risk for
a candidate approximation to $f$ is a key aspect of classifier
design.

Gaussian processes provide a class of models and learning
algorithms for real world problems that have a long history and
are well characterized. Learning algorithms are cast as
minimization problems $min_\mathcal(H) R() $ in a Hilbert space
$ \mathcal{H}$ with a dot product that encapsulates a model and
sample data.  Bayesian methods are often employed for
estimation and inference with Gaussian processes. They allow an
intuitive approach to incorporating prior knowledge in
classification problems and the ability to obtain confidence
intervals for predictions.  Many common regression and
classification algorithms can be cast as minimization problems
in a Reproducing Kernel Hilbert Space (RKHS).

\section{Testing for normality and other distributions }
Powerful inference methods can be employed when data is generated by a Gaussian process. This section describes techniques for testing the normality of a sample and comparing two samples.

Kolmogorov-Smirnov test uses the fact that the empirical
cumulative distribution function is normal in the limit. It is
a non-parametric and distribution free test. Given the
empirical distribution \[F_n(x) = \frac{1}{n}
\sum\limits_{n}^{i=1} \biggl\{\begin{array}{c}1 :x_i\leq x \\0
: x_i>x\\ \end{array}\], and a test CDF\[ F(x)\] the K-S test
statistics are $D_n^+ = max(F_n(x)-F(x) ) $ and $D_n^- =
min(F_n(x)-F(x) ) $ The generality of this test comes at a loss
in precision near the tails of a distribution.  The K-S
statistics are more sensitive near points close to the median,
and are only valid for continuous distributions.

The Kuipers test uses the statistic $D_n^+ + D_n^- $ and is
useful for detecting changes in time series since the statistic
is invariant in ???? transformation of the dependent variable $
F_n$.

The Anderson-Darling test is based on the K-S test and uses the
specific distribution to specify the ????critical values??? of
the test.

The chi-squared is based on the sample histogram and allows
comparison against a discrete distribution, but has the
potential drawback of being sensitive to how the histogram is
binned and requires more samples to be valid.

The Shapiro-Wilk test uses the expected values of the order
statistics of $F(x)$ to calculate the test statistic.  It is
sensitive to data that are very close together, and numerical
implementations may suffer from a loss of accuracy for large
sample sizes.

K-S [Chakravarti, Laha, and Roy, (1967). Handbook of Methods of
Applied Statistics, Volume I, John Wiley and Sons, pp.
392-394].

Shapiro-Wilk [Shapiro, S. S. and Wilk, M. B. (1965). "An
analysis of variance test for normality (complete samples)",
Biometrika, 52, 3 and 4, pages 591-611.]

D’Agostino-Pearson



\section{Regression Methods}
%The multiple regression model in matrix notation can be expressed
%as
%
%revisit - These equations hold for the univariate and the
%multivariate case.
%
%$\textbf{Y} = \textbf{X}{\beta} + \textbf{e}$. %%%%%%%%%%%%%555555
%
%where $\textbf{e} =_d N(\mathbf{\mu},mathbf{\Sigma})$
%
%$E \textbf{Y} = mathbf{\mu} X mathbf{\beta}$
%
%$var \textbf{Y} = \sigma^2 \textbf{I}$
%
%Maximum Likelihood and Least Squares give same estimator;
%
%$\widehat{mathbf{\beta}}=(X^TX)^{-1} X^T Y$
%
%$\hat{\sigma}^2 = frac{1}{n} || Y- \hat{\mu} ||^2$
%
%Multivariate regression is the extension to YM = Xb + e
%
%(called GLM)
%
%Here Y, X, b, and e are as described for the multivariate
%regression model and M is an m x s matrix of coefficients defining
%s linear transformation of the dependent variables. The normal
%equations are
%
%X'Xb = X'YM
%
%and a solution for the normal equations is given by
%
%b = (X'X)-X'YM
%
%Here the inverse of X'X is a generalized inverse if X'X contains
%redundant columns.
%
%Add a provision for analyzing linear combinations of multiple
%dependent variables, add a method for dealing with redundant
%predictor variables and recoded categorical predictor variables,
%and the major limitations of multiple regression are overcome by
%the general linear model. To index
%
%(GLZ) Generalized Linear Model When attempting to explain
%variation in more than one response variable simultaneously the
%modeling exercise is to fit the General Linear Multivariate Model
%(GLMM) to the data. Commonly used multivariate statistical
%procedures such as Multivariate Analysis of Variance (MANOVA),
%Multivariate Analysis of Covariance (MANCOVA), Discriminant
%Function Analysis (DFA), Canonical Correlation Analysis (CCA), and
%Principal Components Analysis (PCA) are all forms of the GLMM.
%When the distribution of the response variable(s) is not normal or
%multivariate normal, or if the variances or the
%variance-covariance matrices are not homogeneous, then application
%of hypothesis tests to GLUM’s or GLMM’s can lead to Type I and
%Type II error rates that differ from the nominal rates.
%Traditionally, transformations of the scale of the response
%variables have been applied to insure that the assumptions
%required for hypotheses tests are met. For example, count data are
%often Poisson distributed and tend to be right skewed.
%Furthermore, the variance of a Poisson random variable is equal to
%the mean of the response. Hence, for count data a transformation
%must both normalize the data and eliminate the inherent variance
%heterogeneity. Commonly, count data are transformed to a
%logarithmic scale or even a square-root scale, however such
%transformations are not always successful in achieving the desired
%end. In fact, there is no a priori reason to believe that a scale
%exists that will insure that data meet the normality and variance
%homogeneity assumptions.The Generalized Linear Model is an
%extension of the General Linear Model to include response
%variables that follow any probability distribution in the
%exponential family of distributions. The exponential family
%includes such useful distributions as the Normal, Binomial,
%Poisson, Multinomial, Gamma, Negative Binomial, and others.
%Hypothesis tests applied to the Generalized Linear Model do not
%require normality of the response variable, nor do they require
%homogeneity of variances. Hence, Generalized Linear Models can be
%used when response variables follow distributions other than the
%Normal distribution, and when variances are not constant. For
%example, count data would be appropriately analyzed as a Poisson
%random variable within the context of the Generalized Linear
%Model. Parameter estimates are obtained using the principle of
%maximum likelihood; therefore hypothesis tests are based on
%comparisons of likelihoods or the deviances of nested models. The
%common linear regression model (a form of the general linear
%model) specifies that the mean response µ is identical to a linear
%function ? of the predictor variables xj:of the predictor
%variables xj:
%
%and uses least squares as the criterion by which to estimate the
%unknown parameters ß   = (ß0,        ß1,...,  ßp)'. When
%observations are independent and normally distributed with
%constant variance s2, least squares estimation of ß   and s2 is
%equivalent to maximum likelihood estimation.
%
%Generalized linear models encompass the general linear model and
%enlarge the class of linear least-squares models in two ways: the
%distribution of Y for fixed x is merely assumed to be from the
%exponential family of distributions, which includes important
%distributions such as the binomial, Poisson, exponential, and
%gamma distributions, in addition to the normal distribution. Also,
%the relationship between E(Y) = \mu and ? is specified by a
%non-linear link function ? = g(µ), which is only required to be
%monotonic and differentiable. The link function serves to link the
%random or stochastic component of the model, the probability
%distribution of the response variable, to the systematic component
%of the model (the linear predictor):
%
% Where g(µ) is a non-linear
%link function that links the random component, E(Y), to the
%systematic component .  For traditional linear models in which the
%random component consists of the assumption that the response
%variable follows the Normal distribution, the canonical link
%function is the identity link. The identity link specifies that
%the expected mean of the response variable is identical to the
%linear predictor, rather than to a non-linear function of the
%linear predictor
%

Standard least squares regression consists in fitting a line
through the data points (training points in learning theory)
that minimizes the sum of square residuals.  The underlying
assumption is that the data and the response can be modeled by
a linear relationship.  In the event that the model accurately
captures the functional dependence of the response generated by
the data, and under the assumptions that the data is corrupted
by Gaussian noise, precise statistical inferences can be made
on the model parameters.

Modifications to this standard model include nonlinear mapping
of the input data, local fitting, biased estimators, subset
selection, coefficient shrinking, weighted least squares, and
basis expansion transformations.

\section{Generalized Linear Models}
Suppose we have $n$ observations of $k$ dimensional data
denoted $\{x_i\}_{i=1}^{k}$ and for each observation we have a
response $y_i$. We wish to fit the observations to the
responses. Generalized Linear Regression is a modeling
technique that allows for non normal distributions and models
non-linear relationships in the training data. M-estimators are
used to fit a generalized linear model Ref Huber (1964).

A linear model $ Y =\Lambda(X)=X\beta + \epsilon$ fits a linear
relationship between the dependent variables $Y_i$ and the
predictor variables $X_i$
\begin{equation}Y_i=\Lambda(X_i)=b_o + b \circ X_i.\end{equation}A generalized
linear model $Y= g(\Lambda(X) ) + \epsilon $ fits the data to $
Y = g (X \circ W)$. Fitting the model consists of minimizing
the objective function $\sum\limits_{i=1}^{n}
g(e_i)=\sum\limits_{i=1}^{n} g(y_i- x_i \beta)$, where $e_i$
are the residuals $y_i-x_i \beta$. We see that for ordinary
least squares $g(e_i)=e_i^2$, and the usual matrix equations
fall out by differentiating with respect to $\beta$. Carrying
this out for general $g$
\begin{equation}
\sum\limits_{i=1}^{n} \frac{\partial  g(y_i-x_i \beta)}{\partial
\beta}=0
\end{equation}
gives the system of $k+1$ equations to solve for estimating the
coefficients $b_i$.  If we set$\alpha(x)=\frac{g'(x)}{x}$ and
calculate the derivative above, we have to solve
\begin{equation}
\sum\limits_{i=1}^{n} \omega(e_i) (y_i-x_i \beta) x_i = 0
\end{equation}
%bbcevisit cast in matrix notation as well.
Which gives rise to a weighted least squares where the weights
depend on the residuals - which depend on the coefficients -
which depend on the weights.  This suggests an iterative
algorithm;
\begin{equation}
\beta^\tau = ( X^{t} W^{(\tau-1)} X )^{-1} X^{t} W^{\tau-1} y
\end{equation}
where $W_{ij}^{(\tau-1)}=\alpha(e_{i}^{(\tau-1})$.

Several parameterizations are popular for the exponential
family. The most general form of the distribution \[
p(x,\theta) = f(x,\theta)e^{g(x,\theta)} \in C^2(\dblr \otimes
\dblr ) \otimes C^2(\dblr \otimes \dblr )\].  The estimators
derived below assume that $f$ and $g$ are separable,
\[ p(x,\theta) = f(x) h(\theta) e^{\alpha(x) \beta(\theta)} \in
C^2(\dblr) \otimes C^2(\dblr ) \otimes C^2(\dblr ) \otimes
C^2(\dblr ) \].

From \[ \int\limits_{x=-\infty}^{x=+\infty} p(x,\theta) dx \:
=1\] we get
\[ \fderiv{\theta}{p(x,\theta)} = 0 = \sderiv{\theta}{p(x,\theta)}d \]
Since the parametrization we have chosen for the exponential
family allows, in the sequel we drop the notation for dependent
variable and denote the derivative with a prime.
\[ \fderiv{\theta}{p(x,\theta)} = \fderiv{\theta}{f h e^{\alpha
\beta}} = h' f e^{\alpha \beta} + f h \alpha \beta' e^{\alpha
\beta} = \bigl( \frac{h'}{h} + \alpha \beta' \bigr) p(x,\theta) \]
which gives
\[\int \fderiv{\theta}{p(x,\theta)} dx = \int \bigl( \frac{h'}{h} + \alpha \beta' \bigr)
p(x,\theta) dx= \frac{h'}{h} \int p(x,\theta) dx \:+\: \beta' \int
\alpha(x) p(x,\theta) dx = \frac{h'}{h}+\beta' E[\alpha(x)] \] so
that \[E[\alpha(x)]=-\frac{h'}{h \beta'}\]. Continuing along
this vein,\begin{gather*} 0 =\int \sderiv{\theta}{p(x,\theta)}
dx = \int \fderiv{\theta}{\bigl(
\frac{h'}{h} + \alpha \beta' \bigr) p(x,\theta) } dx =\\
 \int \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+\alpha \beta''
\bigl ) p(x,\theta) + (\frac{h'}{h}+\alpha \beta')
\fderiv{\theta}{p(x,\theta)} \: dx = \\
\int \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+\alpha \beta''
\bigl ) p(x,\theta) + (\frac{h'}{h}+\alpha \beta')^2
p(x,\theta) \: dx =
\\ \int \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+\alpha \beta'' \bigl )
p(x,\theta) + (\frac{h'}{h}+\alpha \beta')^2 p(x,\theta) \: dx
=
\\ \int \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+\alpha \beta'' \bigl )
p(x,\theta) + (\alpha \beta'- E[\alpha(x)]\beta')^2 p(x,\theta)
\: dx
\end{gather*}  Keeping in mind that
 \[ Var[a x ]= E[ (ax-E(ax)^2 ] = a^2 E [ (x-E[x])^2] =a^2 Var[x]
 \]  we get the variance via\[ \bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+E[\alpha(x)] \beta'' \bigl
)+ Var[\alpha(x) \beta'(\theta)] =
\bigr(\frac{h''}{h}-\frac{(h')^2}{h^2}+E[\alpha(x)] \beta''
\bigl )+ (\beta')^2 Var[\alpha(x)] = 0 \]

The score $U(x)$ is given by \[
U(x)=\pfderiv{\theta}{L(\theta,x)} = \pfderiv{\theta}{\log \:
p(x,\theta)}=\pfderiv{\theta}{\bigr( \log h(\theta) + \log f(x)
+ \alpha(x) \beta(\theta)\bigl) } = \frac{h'}{h} + \alpha
\beta'\] so
\[E[U(x)]=\beta'E[\alpha(x)] + \frac{h'}{h} =0 \].  The Fisher
Information $\mathcal{F}$ is defined \[\mathcal{F}=Var[U(x)]
=Var[ \alpha \beta' + \frac{h'}{h}]= Var[ \alpha \beta'] \] So
from above we have \[Var[U(x)]= Var[ \alpha \beta']
=\bigr(-\frac{h''}{h}+\frac{(h')^2}{h^2}-E[\alpha(x)] \beta''
\bigl )\].  Now differentiating,  \[
\fderiv{\theta}{U(\theta,x)} = \frac{h''}{h} -
\frac{(h')^2}{h^2} + \alpha \beta''\]
\[E[U'(\theta,x)]=\frac{h''}{h} - \frac{(h')^2}{h^2} + E[\alpha]
\beta'' =  \frac{h''}{h} - \frac{(h')^2}{h^2} -\frac{ \beta''
h'}{\beta'}= - Var[U(x)] \].  Note that if we write the
parametrization of the separable exponential family as
\[ p(x,\theta) =  e^{\alpha(x) \beta(\theta)+\log(f(x))+\log(h(\theta))}\]
then,
\[\sderiv{\theta}{\log(h(\theta))}=\fderiv{\theta}{\frac{h'}{h}}=\frac{h''}{h}-\frac{(h')^2}{h^2}\].




%bbcrevisit - this duplicates and has notation clash with material above.
A general form of the exponential distribution
\begin{equation}
\rho(x;\theta) = exp( \frac{x \theta - \xi(\theta) }{\sigma} ) \nu
( x)
\end{equation}
has a log likelihood for a random sample $\{ X_i  \}_{i=1
\hdots N}$ given functionally by
\begin{equation}
\mathcal{L} (\theta) =  \sum\limits_{i=1}^{N} [ X_i  \theta  - \xi
(\theta) + log   ( \nu ( X_i ) ) ]
\end{equation}
The scale parameter $\sigma$ and $\theta$ are orthogonal
parameters in that E [ ] The Generalized Linear model can

$\rho'$ is referred to as a link function in the statistical
literature.  If $\rho'(x)= x\field(1)$ and
$\epsilon=(\epsilon_1, \hdots ,\epsilon_n)$ are iid
$N(\mu,\sigma)$ we have multiple linear regression.  In
classification problems or binomial models the logit
$\rho'(x)=log(x/(1-x))$ link function is used. The logit is
extended to the $k$ category case by \begin{equation}\rho'( x_i
| x_j j \neq i)= log ( \frac{x_i}{1- \sum\limits_{j \neq i}
x_j})\end{equation}. The posterior probability densities
${p_i(?)}$ bbcrevisit (or $p_i$ the probability of observing
class $i$)  of k classes are modeled by linear functions of
the input variables $x_i$.

%\subsection{Multinomial and ordinal Regression}
%When dealing with nominal data, one can consider all classes at
%once via the , or model
%A logistic model for predicting $P(Y | X)$ has a response
%\begin{equation} p(y|x)\equiv p(x) = \frac{1}{1+e^{- x \circ w}
%}\end{equation}. A quick calculation shows that the variance of
%our model is given by $\sigma^{2} = \int p(y|x) (x-\mu)^2 dx$. The
%variance is $p(x)(1+p(x))$

\section{Fitting the GLM}
Iteratively re-weighted least squares (IRLS) is used to for fitting generalized linear models and in finding M-estimators.  The objective function

\begin{equation}
J(\beta^{i+1}) = arg min \sum w_i ( \beta) | y_i - f_i (\beta) |
\end{equation}

is solved iteratively using a Gauss-Newton or Levenberg-Marquardt (LM) algorithm. LM is an iterative technique that finds a local minimum of a function that is expressed as the sum of squares of nonlinear functions. It is a combination of steepest descent and the Gauss-Newton method. When the current solution is far from the minimum the next iterate is in the direction of steepest descent. When the current solution is close to the minimum the next iterate is a Gauss-Newton step.

Linear least-squares estimates can behave badly when the error is not normal.  Outliers can be removed, or accounted for by employing a robust
regression that is less sensitive to outliers than least squares.  M-Estimators were introduced by Huber as a generalization to maximum likelihood estimation.  Instead of trying to minimize the log likelihood

\begin{equation}
L(\theta) = \sum - log ( p(x_i, \theta)
\end{equation}

Huber proposed minimizing

\begin{equation}
M(\theta) = \sum  \rho(x_i, \theta)
\end{equation}

where $\rho$ reduces the effect of outliers. Common loss function are the Huber, and Tukey Bisquare.  For $\rho(x) = x^2$ we have the familiar least squares loss.

M estimators arise from the desire to apply Maximum Likelihood
Estimators to noisy normal data, and to model more general
distributions. They provide a regression that is robust against
outliers in the training set, and allow for modeling of
non-Gaussian processes. When $\rho$ above is a probability
distribution, we are preforming a maximum likelihood
estimation.

The Huber function which is a hybrid $L^2$ $L^1$ norm
\begin{equation}
\rho_\eta(e_i)=\biggl\{\begin{array}{cc}
\frac{e_i^2}{2} & |e_i| \leq \eta \\
  \eta |e_i| - \frac{\eta^2}{2} & |e_i| > \eta \\
\end{array}
\end{equation}
The  Tukey Bisquare estimator is given by
\begin{equation}
g_\eta(e_i)=\biggl\{ \begin{array}{cc} \frac{\eta^2}{6} (
1-[1-\frac{e_i}{\eta}_2]^3) & |e_i| \leq \eta \\
\frac{\eta^2}{6} & |e_i| > \eta \\
\end{array}
\end{equation}

Numerical procedures for doing this calculation are the
Newton-Raphson method [see the section on root finding below ],
and Fisher-Scoring method [ replace $ \frac{\partial^2
\mathcal{L}(\mathbf{\theta})}{\partial \mathbf{\theta} \partial
\mathbf{\theta}^{t} }$ with $E[ \frac{\partial^2
\mathcal{L}(\mathbf{\theta})}{\partial \mathbf{\theta} \partial
\mathbf{\theta}^{t} }  ]$. For high dimensional data, many
models may be fit in an attempt to find the simplest one that
can explain the data.

In the language of statistical learning theory, the choice of a
norm $\rho$ is tantamount to choosing a loss function.
Restricting the admissible functions to the one parameter
family of exponential probability distributions defines the
capacity via a functional form of the law of large numbers.
\cite{Scholkopf B. (2002)}


\section{Feature Subset Selection (FSS)}
The goal of feature selection techniques to to improve the model building process by eliminating features that do not have discriminative power. Algorithms for feature selection either rank features or create subsets of increasing optimality.  FSS should be contrasted with feature extraction techniques such as PCA, LLE, or Laplacian eigenmaps.  The goal of feature extraction is to transform data from a high dimensional space to a low dimensional one while preserving the relevant information.

The statistical approach to feature selection most commonly used is stepwise regression.  Common optimality criteria are FS schemes the Kolmogorov-Smirnov Test ,the t-test, the f-test, the Wilks Lambda Test and Wilcoxon Rank Sum Test.

Feature subset selection (FSS) is the process of determining which measurements will be used for classification.  It's important to distinguish this process from a data dimension reduction process such as PCA which requires all the original measurements to compute the projection. The better FSS algorithms are recursive

Construct a $p x M$ basis matrix $H^{T}$ and transform feature vector $x' = H^{T} x$.

Generalize to $L^{2}$ with smoothing splines

Smoothing spline $RSS(f,\lambda)= \sum\limits_{i=1}^{N} (y_{i}
-f(x_{i}) )^{2} + \lambda \int f''(t)^{2} dt$. where $f \in
C^{2}(\field{R} )$ This is minimized in $L^{2}$ the first term
measuring closeness of fit, and the second term penalizes
curvature. $\lambda \rightarrow 0$ gives any function
interpolating the data points ${x_i}_{i  \in {1, ... N} } $ an
$\lambda \rightarrow \infty$ constrains $f$ to be linear.


\section{Longitudinal Data Analysis} Longitudinal data analysis
is the observation of multiple subjects over repeated
intervals. Binary repeated responses are typically modelled
with a marginal or random effects model, which will be made
precise below. Marginal Models are a generalization of the GLM
presented above for correlated data.  Here, the correlation is
inter subject across time.  Statistical analysis of
longitudinal data must take into account that serial
observations of a subject are likely to be correlated, time may
be an explanatory variable, and that missing response data my
induce a bias in the results.

Let ${X_{ij}}$ be time varying or fixed covariates for the
binary response ${Y_{ij}}$ of subject $i \in {1,...n}$ at time
intervals $j \in {t_1,...t_m}$. By convention $X_{ij} \in
\field{R} x \field{R^p}$ where the first dimension is the
intercept. The marginal model is; $logit (E(Y_{ij} | X_{ij}) )
= X_{ij}^{\dagger} \beta$ and enforces the assumption that the
relationship between the covariates and the response is the
same for all subjects. Recall that for a binary response,
$E(Y_{ij} | X_{ij}) = P(Y_{ij}=1 | X_{ij})$.  The random
effects model takes into account that the relationship between
the covariates and response varies between subjects; $logit
(P(Y_{ij}=1 | X_{ij}) ) = X_{ij}^{\dagger} \beta_i$ If it is
know that only a subset of the covariates are involved in the
inter-subject variability, we can set $\beta_i= \beta +
\beta_i$ and write $logit (P(Y_{ij}=1 | X_{ij}, \beta_i) ) =
X_{ij}^{\dagger} \beta + O X_{ij} \beta_i$ Where the kernel of
$O : \dblr^n \rightarrow \dblr^{n'}$ is the span of the
covariates that do not change between subjects.  If $\lambda_i
=_d N(0,\sigma)$ then the difference in the parameter vectors
$\beta$ in the two models differ according to $\sigma$.

The GEE method of fitting the marginal model is described in:
\cite{Liang, K-Y and Zeger, S. L.(1986)}

The Survival Analysis is a form of longitudinal analysis that
takes into consideration the amount of time an observation is
made on a subject.

GLM's can be used to fit discrete longitudinal hazard models
derived from survival analysis, see  \cite{Prentice and
Gloeckler (1978)}.   \cite{Meyer, B.D. (1990)} generalized that
approach to account for an unobserved subject heterogeneity.

\cite{Holmen, M (2005)} applied the hazard model of
\cite{Prentice, R. and L. Gloeckler (1978)} to the takeover
hazard of large firms.  A negative relationship between dual
class ownership and value is empirically known, and that
relationship can be explained by the lower takeover probability
of the dual class firms.  Dual class entities had a higher risk
for takeover, but the hazard is lower since these firms use the
dual class structure to change the capital structure in a way
that allows the controlling shareholders to remain in control
by reducing firm value.

The proportional hazards model can be discretized, but it is
important to identify whether the process is truly a discrete
process.  In that case the link function should be the logit as
the Marginal Model above specifies, rather than the log-log
function of the discretized proportional model.  The difference
is the modelling of a probability transition in the former case
versus a rate for the latter case.

Variable selection techniques for longitudinal data are
relatively limited and most seem to rely on Wald type tests.
Wald tests to include a variable are based on already computed
maximum likelihood values. The Rao score test is used to
include a covariate in the model building process.  The Wald
test calculates
\[z^2=\frac{\widehat{\beta}}{stderr}=_d  \chi^2\]

The likelihood ratio statistic for comparing two models $L_0
\in L_1$ \[-2 \frac{L_0}{L_1} =_d \chi^2\] is useful for
backward stepwise variable subset selection. The degrees of
freedom of the of the statistic is equal to the difference in
dimension of the two models.

\section{Discretization \& Sheppard's Correction}
W. Sheppard (1898) Derived an approximate relationship
between the moments of a continuous distribution and it's
discrete approximation. This provides a transformation to
statistical estimators that correct for the binning of
continuous data.  As the scale at which datum are collected is
increased, the variance of an estimate can become biased.

It is important to assess  bias caused by grouping and to correct it if necessary.
The  bias of the approximate maximum likelihood estimator where
observations are approximated by interval midpoints $O(w^2)$, where $w$ is the bin width. A Sheppards correction can be used to reduce the bias to order $O(w^3)$,

Signal processing engineers often have to deal with such a
quantization effect when designing finite precision systems,
image processing being a particularly relevant example. The
engineering community typically models the quantization noise
$Q=[X]-X$, where $[X]$ is the quantized realization of $X$. One
might be tempted to apply a Sheppard's correction to the
moments of the quantized data, thinking that $Var(X)<Var([X])$
but it is possible to construct examples where $Q$ and $[X]$
are independent, or where $Cov(X,Q)$ is such that
$Var(X)>Var([X])$.

Shepard's correction is limited in that is doesn't apply to the
first moment, and the frequencies of the first and last bins
need to be low.

Expand $p(x;\theta)$ in a Taylor series and substitute in the
Maximum Likelihood equations. \cite{Lindley, D. V. (1950)}

Suppose we have n realizations of iid RV's ${X_1, \hdots ,
X_n}$ and the data is collected on a discrete grid on the range
of $X$ $Ran(X)=\{[y_i-d_i/2,y_i+d_i/2]\}_{i=1}^{i=m}$ where the
intervals are centered on the location where a measurement. The
realized values ${y_1, \hdots , y_m}$ have probabilities
$p_i=\int\limits_{y_i - d_i /2}^{y_i+d_i /2} p(x;\theta) \;\;
dx$ Expanding $p(x;\theta)$ in a Taylor series about $y$,
$p(x;\theta)= \sum\limits_{i=0}^{\infty} \frac{p^{(i)}(y) }{i!}
(x-y)^i$.


\section{Multidimensional Scaling}
Multidimensional scaling (MDS) is an alternative to factor analysis. The aim of MDS and factor analysis of the analysis is to detect meaningful underlying dimensions that explain similarities or dissimilarities data points. In factor analysis, the similarities between points are expressed via the correlation matrix. With MDS any kind of similarity or dissimilarity matrix may be used.

Given $n$ observations ${x_i}_{i=1}^{n} \in \dblr^k$ and $n^2$
distances $d_{ij}$ between them, MDS looks for $n$ points
${\xi_i}_{i=1}^{n}$ in $dblr^l : l<k$ that preserve the
distance relations. When a metric $\rho()$ exists for the similarity measure, gradient
descent is used to minimize the MDS functional $S(\xi_1, \ldots
, \xi_l)=\biggl( \sum_{i \neq j}
d_{ij}-||\xi_i-\xi_j||_{\rho}\biggr)^\frac{1}{2}$.

\section{Principal Components} For a data set $\textbf{X} \in
M_{(N,m)}(\mathbb{R}) = { x_1, x_2, \ldots x_N | x_i \in
\mathbb{R}^m } $, the first k principal components provided the
best k dimensional linear approximation to that data set.
Formally, we model the data via $f(\theta) = \mu + \textbf{V}_k
\theta | \mu \in \mathbb{R}^m, V_k \in O_{m,k}(\mathbb{R}),
\theta \in \mathbb{R}^k$ so $f(\theta)$ is an affine hyperplane
in $\mathbb{R}^m$

\section{Evaluating classifier performance} Multi-class problems
can be treated simultaneously or broken in to a sequence of two
class problems.  Cross validation is used both for classifier parameter tuning
and for feature subset selection.  Student-t and ANOVA can be used to evaluate the performance of classifiers against one another.  The Student-t test compares
two classifiers, while the ANOVA test can compare multiple
classifiers against one another.  Confusion matrices and ROC graphs are commonly employed visualization tools for assessing classifier performance. The
rows of a confusion matrix add to the total population for each
class, and the columns represent the predicted class.  An ROC
curve plots the TP rate against the FP rate. Often a curve in
ROC space is drawn using classifier parameters for tuning
purposes.

\begin{table}[h]
\begin{tabular}{|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  TN &  FP \\
  \hline
  FN & TP \\
  \hline
\end{tabular}
\caption{Two class confusion matrix where the proportions are
specified}
\end{table}

Common performance metrics for the two class problem are
sensitivity (TP), specificity (TN), precision (the proportion
of predicted cases within a class that were correct), and
accuracy (the overall proportion of correct predictions). These
metric can be extended to more than two classes by defining
$A=tr ( C ) / || C ||_{L^\infty}$ where $C$ is the confusion
matrix. TP, FN, FP, TN are proportions defined for the two
class problem.



\section{Graph Spectra}
Graph spectral methods are some of the most successful heuristic approaches to partitioning algorithms in solving sparse linear systems, clustering and, ranking problems.  Eigenvalues of the graph Laplacian are used to transform a combinatorial optimization problem to a continuous one, typically a SDP problem.  Recent advances in SDP optimization techniques have opened new avenues of research in combinatorial optimization.  For instance, isoperimetric properties of a graph are used to find efficient communication networks, and fast convergence of Markov Chains.

\section{Matrix Factorization}
Many forms of matrix factorization can be cast as an optimization problem that involves minimization of generalized Bregman divergences\cite{BDAUnifiedViewMatrixFactorizationModels}.  Factorization algorithms such as  NNMF, Weighted SVD, Exponential Family PCA, , pLSI, Bregman co-clustering \cite{CCBanerjee04ageneralized} can be cast in this framework.  The approach uses an alternating projection algorithm for solving the optimization problem which allows for generalizations that include row, column, or relaxed cluster  constraints.  A brief description of the algorithm is given below.  The description of a generalized Bregman divergence can be found in \cite{BDGordon99approximatesolutions}.

\section{PCA and its generalization to the Exponential Family}
Here we describe a generalization  of Principal component analysis (PCA) to the Exponential Family of probability distributions.  PCA is a popular dimensionality reduction technique that seeks to find a low-dimensional subspace passing close to a given set of points \begin{equation*}\{x_i\} \subset \mathbb{R}^n\end{equation*}.  The procedure is to solve the optimization problem that minimizes the sum of squared differences of the data points to the projections on a subspace spanned by the empirical variance after centering the data to have mean $0$;
\begin{equation*}
\sum\limits_{i=i}^{n} \norm{x_i - \theta_i}^2_{\ell^2}
\end{equation*}.  The choice of $\ell^2$ norm here codifies the assumption of Gaussian data.  An alternate interpretation of the algorithm is finding the  parameters ${\theta_i}$ that maximizes the log likelihood of the data which corresponds to \begin{equation*}
\sum\limits_{i=i}^{n} \norm{x_i - \theta_i}^2_{\ell^2}
\end{equation*}.  The goal of PCA is to find the the true low dimensional distribution   of the data given the assumption that data is corrupted by Gaussian noise.
Bregman divergences
  \begin{equation*}
  D_\phi(A,B)=\phi(A)-\phi(B) - \nabla \phi(B) (A-B)
\end{equation*}
offer a framework to extend PCA [and other spectral dimension reduction techniques] to the entire Exponential Family.  Here $\phi$ is a striclty convex function.  The roles of

Let  $\theta_i$ be the natural parameter for dimension $i$, with Exponential distribution $P_\theta$.  Then the conditional expectation is given by
\begin{equation*}
log P_\theta (x| \theta) = log P_0(x) + x \theta - G(\theta)  \sp : G \ni \int{ P_\theta dx} =1
\end{equation*}
We can model multivariate data where the conditional distribution can vary along the feature space.  The common feature of this PCA model and GLZ regression is the derivative of $G$ which is familiar link function and the loss function which is appropriate for $P_\theta (x | \theta)$.  The non linear relationship in the GLZ regression model data is captured by the link function $h = \frac{d}{d \theta}G(\theta)'$.  This feature is also passed on to the generalized PCA.  Instead of projecting on to a linear subspace, a Bregman divergence is used as the distortion measure.  This gives a convex optimization problem to solve which can be shown to converge.  In \cite{BDAzoury99relativeloss} a dual function  to  $\phi$ is defined by the relationship $\phi(g(\theta))+G(\theta)=h(\theta) \theta$ which is used to write the log likelihood as a Bregman divergence
\begin{equation*}
    \log P( x | \theta ) = -log P_0(x) - \phi(x) + D_\phi (x,h(\theta))
\end{equation*}.  Typically $x$ is a vector but extending to matrices is straightforward.




\section{Manifold Learning}
There are numerous machine learning techniques which accomplish some form of dimensionality reduction.
Manifold learning uses principal curves and manifolds to encode a natural geometric framework for nonlinear dimensionality reduction.  These methods construct low-dimensional data representation using a cost function that retains local properties.
Contrasting methods such as MDS employ proximity data via a similarity or distance matrices.  The important ISOMAP \cite{MDS_ISOMAP}algorithm extends MDS by capturing geodesic measurements of non-local pairs on the data manifold $M$ via an multi-scale approximation.  Non-local distances are approximated via a shortest path on a K nearest neighbor clustering of the data.  Effectively a ball in data space is used to represent a cluster, and a graph is then constructed to encode the non-local information.  The connectivity of the data points in the neighborhood graph are the nearest k Euclidean neighbors in the feature space.  Dijkstra's algorithm for computing shortest paths with weights is used to construct the proximity matrix from the neighborhood graph.  The top n eigenvectors encode the coordinates in the low dimensional Euclidean space.  Choosing the correct number of neighbors is an essential component to an accurate representation.
Other shortest path algorithms that may be employed to calculate the geodesic distances are listed below:
\begin{itemize}
  \item Dijkstra's algorithm finds the single-pair, single-source, and single-destination shortest path.
  \item Johnson's algorithm finds all pairs shortest paths
  \item Bellman-Ford algorithm single source problem and allows negative edge weights.
  \item Floyd-Warshall algorithm solves all pairs shortest paths.
  \item A* search algorithm solves the single pair shortest path problem.
\end{itemize}
In \cite{MDSBernstein00graphapproximations} a sampling condition is given which bounds the quality of the manifold embedding based on the quality of the neighborhood graph.
%
%In the figure below find several common embedding from
%\begin{equation*}
%\mathcal{R}^3 \mapsto \mathcal{R}^2
%\end{equation*}
%of a simple swiss roll data set
%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=15cm]{Figures/NNDM/swiss_roll2.pdf}
%  \caption{2 Classes}
%\end{figure}
%
%
%
%\section{Figures}
%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/2_class.pdf}
%  \caption{2 Classes}
%%\end{figure}
%%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/2_class_FeidlerVectorPartition_2eigVec.pdf}
%  \caption{2 Classes cut with Feidler Vector}
%%\end{figure}
%%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/2_class_graph_2ndEigenvect_FeidlerVectorPartition_2eigVec.pdf}
%  %\caption{2 Classes - FeidlerVector}
%\end{figure}
%
%
%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/GramMatrix_Euclidian_3class_GaussianMixture_2dim.pdf}
%  \caption{2 Classes}3
%%\end{figure}
%%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/3Class/3_classespdf.pdf}
%  \caption{2 Classes}3
%%\end{figure}
%%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/3Class/3_classes_adjacency_Matrix.pdf}
%  \caption{2 Classes}3
%\end{figure}
%\begin{figure}
%%  % Requires \usepackage{graphicx}
%%  \includegraphics[width=6cm]{Figures/3Class/GramMatrix_Euclidian_3class_GaussianMixture_2dim.jpg}
%%  \caption{2 Classes}3
%%\end{figure}
%%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/3Class/3_classe_FMMC_VertexWeight3ndEigenvector.pdf}
%  \caption{2 Classes}3
%%\end{figure}
%%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/3Class/3_classes_2nd_eigenvector.pdf}
%  \caption{2 Classes}3
%%\end{figure}
%%\begin{figure}
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=6cm]{Figures/3Class/3_classes_3nd_eigenvector.pdf}
%  \caption{2 Classes}3
%\end{figure}

\section{Graph Laplacian}
\cite{GLPBoydconvexoptimization}, \cite{GLPChung93laplaciansof}, \cite{GLPCrescenzi96toweight},
\cite{GLPGuatterygraphembeddings}, \cite{GLPBoydconvexoptimization},
\cite{GLPChung93laplaciansof},
\cite{GLPSpectralAnalysisComplexLaplacianMatrices}, \cite{GLPKellersignedgraph}

Let $G$ be a connected simple graph with vertex set $V = {1, 2, ... , n}$ , edge set $E$ and let each edge be associated with a positive number, called the weight of the edge. The above graph is called a weighted graph. An unweighted graph is just a weighted graph with each of the edges bearing weight 1.  The weight $w(i)$ of a vertex $V_i$ is the sum of the weights of the edges incident with it. There are a number of ways in which the  Laplacian matrix $L$ is defined; the combinatorial Laplacian, the normalized Laplacian and the unsigned Laplacian.  Spectra from graph matrix representations may be obtained from the adjacency matrix $A$ and the various Laplacian discretizations.  Spectra can also be derived from the heat kernel matrix and path length distribution matrix.

The matrix representation of the graph Laplacian has a significant effect on the spectrum.  Attributes may be accounted for by by a complex number that encodes the edge attributes.  The node attributes may be encoded in the diagonal elements.   The complex graph Laplacian matrix is Hermitian, and hence it has real eigenvalues and complex eigenvectors.  Graph feature vectors can be embedded in a pattern space by PCA, MDS, and LDA( linear discriminant analysis).  Attribute graphs may be characterized by the application of symmetric polynomials to the real and complex components of the eigenvectors. \cite{GLPAnaFred} This gives rise to permutation invariants that can be used for pattern vectors.  Partitioning a graph into three pieces, with two of them large and connected, and the third a small separator set can be accomplished using the second eigenvector [the Feidler Vector] of the graph Laplacian.  In the case or sparse graphs, the first few eigenvectors can be efficiently computed using the Lanczos algorithm [see section below on ARPAC].  This graph partitioning algorithm can be extended to give a hierarchical subdivision of the graph.

\section{Learning With Kernels}
\cite{KLBurges98atutorial}, \cite{KLKeerthi99improvementsto}, \cite{KLProgramminglearningthe},
\cite{KLScholkopf00newsupport}, \cite{KLScholkopf00statisticallearning}, \cite{KLShevade99improvementsto},
\cite{KLTsang03distancemetric}, \cite{KLWeston00featureselection}, \cite{KLSchultz03learninga}

Kernel learning is a paradigm for classification and regression where prior belief is expressed in the construction of a similarity matrix of distanced between points in a feature space $\Omega$ by embedding via a non linear map $\phi$ in a higher [often infinite] dimensional Hilbert space using the kernel as an inner product.
\begin{center}\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
  K(x,x')= <\phi(x),\phi(x')> \\
  K \succeq 0 \\
  SPD \Rightarrow \sum\limits_{x \in \Omega}^{}  \sum\limits_{x' \in \Omega}^{} f(x) K(x,x') f(x') \geq 0 \forall f\ \in \ell^2(\Omega)
\end{eqnarray*}\end{center}
Recall that infinitely divisible probability distributions aries as the sum of $iid$ random variables.  Infinitely divisible kernels have the representation
\begin{center}\begin{eqnarray*}
K=K^{\frac{1}{n}} \ldots K^{\frac{1}{n}} \\
K= e^{\beta H} \\
e^{\beta H} = \lim\limits_{n\rightarrow \infty} (1+ \frac{\beta H}{n} )^n
\end{eqnarray*}\end{center}
We construct a mutli-resolution representation of the data with exponentiated kernels.  The sequence of kernels $K(\beta)$ represents a one parameter group associated with a diffusion on the graph of the data.  A $\beta \rightarrow  0 \infty$ the kernel moves from the identity to one that represents the clusters in the off diagonal components.  The local structure of $\Omega$ is preserved in $H$ while the global geometry of the data set is progressively revealed in $K(\beta)$ as we push the diffusion forward with the one parameter group.  We can construct exponentiated kernels over direct products of sets $\Omega_1 \bigotimes \Omega_2$ that will allow for the class conditional representation [bbcrevisit term use multiclass].  Simply set $H = H_1 \bigotimes I_{\Omega_1} +  H_2 \bigotimes I_{\Omega_2}$.
\begin{center}\begin{eqnarray*}
K(\beta) = e^{\beta H} =  e^{(\beta H_1 \bigotimes I_{\Omega_1} +  H_2 \bigotimes I_{\Omega_2})} \Rightarrow \\
\frac{d}{d \beta} K(\beta) =  H (K_1(\beta) \bigotimes K_2(\beta))
\end{eqnarray*}\end{center}
The kernels thus  constructed can be used to drive a diffusion on a graph by letting $H$ be the familiar graph Laplacian.  Furthermore, the continuum limit of infinite data can be analyzed in within the framework of a discreet stochastic process much the way the convergence of finite element solutions of PDE's takes place.

%\section{PCA \& ICA}
PCA finds linear combinations of the variables that correspond to directions of maximal variance in the data. Typically this is performed via a singular value decomposition (SVD) of the data matrix $A \in R^{n,m}$, or via an eigenvalue decomposition if A is a covariance matrix in which case $A \in R^{n,n}$. Representing the data in the directions of maximum variance allows for a dimension reduction that preserves information. Principal component directions are uncorrelated which can be useful.  PCA has the disadvantage that components are usually linear combinations of all variables. Weights in the linear combination data elements are non-zero. Sparse PCA is an attempt to find a low dimension representation of the data that explainers most of the variance.


%\section{Metric Multidimensional Scaling}
%\cite{MDSCuadras95metricscaling}, \cite{MDSProjectmetricmultidimensional},
%\cite{MDSSeDuMi99selfdualminimization}, \cite{MDSSuilearningdistance}, \cite{MDSTROSSET97distancematrix},
%\cite{MDSTsang03distancemetric}, \cite{MDSXing03distancemetric}, \cite{MDSYoung85multidimensionalscaling}
%
%\section{Convex Optimization}
%\cite{COAlizadeh96primaldualinteriorpoint}, \cite{COBenson97solvinglargescale},
%\cite{COHelmberg94aninteriorpoint},
%\cite{COLanckriet04learningthe}, \cite{COLewis96eigenvalueoptimization},
%\cite{COSturm95symmetricprimaldual}, \cite{COSturm99usingsedumi}, \cite{COToh99sdpt3},
%\cite{COVandenberghe95aprimaldual}, \cite{COVandenberghe96connectionsbetween},
%\cite{COVandenberghe96semidefiniteprogramming}, \cite{COWeinberger04unsupervisedlearning},
%\cite{COXiao04fastlinear}
%
%\section{Sparse Methods in Spectral Graph Partitioning}
%\cite{SPAlthofer93onsparse}, \cite{SPAwerbuch90sparsepartitions}, \cite{SPChandra95newsparseness},
%\cite{SPChung01thediameter}, \cite{SPEppstein00spanningtrees}, \cite{SPFernholzsparserandom},
%\cite{SPSfbgraphpartitioning}, \cite{SPYuster97independenttransversals}
%
%\section{Laplacian Eigenvectors an Faber Krahn inequalities }
%\cite{FLDavies01discretenodal}, \cite{FLReidys01combinatoriallandscapes}, \cite{FLReidys93neutralityin},
%\cite{FLRockmore01fastfourier}, \cite{FLStadler03landscapesand}
%
%
%\section{Non Negative Matrix Factorization}
%\cite{NNMFGuillamet02determininga}, \cite{NNMFHeiler06learningsparse}, \cite{NNMFHoyer04nonnegativematrix},
%\cite{NNMFLectureuseof}, \cite{NNMFLEE01algorithmsfor}, \cite{NNMFLEE01algorithmsfor},
%\cite{NNMFsemisupervisedmultilabel}, \cite{NNMFTheisfirstresults},
%\cite{NNMFereira93distributionalclustering}
%Unsupervised learning algorithms such as principal components analysis and vector quantization can be understood as factorizing a data matrix subject to different constraints. Depending upon the constraints utilized, the resulting factors can be shown to have very different representational properties. Principal components analysis enforces only a weak orthogonality constraint, resulting in a very distributed representation that uses cancelations to generate variability [1, 2]. On the other hand, vector quantization uses a hard winner take all constraint that results in clustering the data into mutually exclusive prototypes [3]. We have previously shown that nonnegativity is a useful constraint for matrix factorization that can learn a parts representation of the data [4, 5]. The nonnegative basis vectors that are learned are used in distributed, yet still sparse combinations to generate expressiveness in the reconstructions [6, 7]. In this submission, we analyze in detail two numerical algorithms for learning the optimal nonnegative factors from data.


\section{Bregman Divergences}
NNMA is the approximation of a non-negative matrix $A$ by a low rank matrix $BC$ where $B\succ 0$ and $C\succ 0$.  Bregman divergences are a robust distortion measure for this matrix factorization.  Formally $D_\phi(A,BC)=\phi(A)-\phi(BC) - \nabla\phi(BC) (A-BC)$ measures the quality of the factorization relative relative to a convex penalty function.

Modeling of relational data can be abstracted out to the factorization in to a low dimensional representation of a data matrix $(X_ij)$ where links [or relations] are represented as an $n x m$ matrix $X$ where $X_{i,j}$ indicates whether a relation exists between entities of type $i, j$.  Let $f$ be a link function and $X^{~}$ be a factorization of $X$ into a low rank approximation $X \approx U V^T : U \in R^{m x k}, v \in R^{m x k}$.  The link function $f$ can be interpreted as in $GLM$ which gives extends exponential models to matrices.  A simple example is choosing the identity link which and minimizing in the $\ell^2$ norm gives rise to the SDV and the Gaussian model for the data ${X_ij}$.  Similarly we can  extend to Bernoulli, Poisson, Gamma, error distributions.


\section{Matrix Factorization via Generalized Bregman Divergences}
Many forms of matrix factorization can be cast as an optimization problem that involves minimization of generalized Bregman divergences\cite{BDAUnifiedViewMatrixFactorizationModels}.  Factorization algorithms such as  NNMF, Weighted SVD, E xponential Family PCA, , pLSI, Bregman co-clustering \cite{CCBanerjee04ageneralized} can be cast in this framework.
 The approach uses an alternating projection algorithm for solving the optimization problem which allows for generalizations that include row, column, or relaxed cluster  constraints.  A brief description of the algorithm is given below.  The description of a generalized Bregman divergence can be found in \cite{BDGordon99approximatesolutions}.


\section{Spectral Clustering}
\cite{SCBrand03aunifying}, \cite{SCDhillon03informationtheoreticcoclustering},
\cite{SCKannan00onclusterings},
\cite{SCNg01onspectral}, \cite{SCSchplkopf98nonlinearcomponent}, \cite{SCWeiss99segmentationusing}

\section{Co-Clustering}
\cite{CCAchtert06mininghierarchies}, \cite{CCageneral},
\cite{CCAndahierarchical}, \cite{CCBanerjee},
\cite{CCBanerjee04ageneralized},
\cite{SCDhillon03informationtheoreticcoclustering},
\cite{CCFredericoabipartite},
\cite{CCHuang06informationmarginalization},
\cite{CCHucoclusteringbipartite},
\cite{CCHofmann99probabilisticlatent},
\cite{CCHupreservingpatterns},
\cite{CCMerugu06multiwayclustering}, \cite{}
%Spectral clustering (Ng et al., 2001; Bach \& Jordan, 2004) has been
%well studied in the literature. The spectral clustering methods
%based on the graph partitioning theory focus on finding the
%best cuts of a graph that optimize certain predefined criterion
%functions. The optimization of the criterion functions usually
%leads to the computation of singular vectors or eigenvectors of
%certain graph affinity matrices. Many criterion functions, such
%as the average cut (Chan et al., 1993), the average association
%(Shi \& Malik, 2000), the normalized cut (Shi \& Malik, 2000),
%and the min-max cut (Ding et al., 2001), have been proposed.
%Spectral graph partitioning has also been applied to a special
%case of multi-type relational data, bi-type relational data
%such as the word-document data (Dhillon, 2001; H.Zha \&
%H.Simon, 2001). These algorithms formulate the data matrix as a
%bipartite graph and seek to find the optimal normalized cut for
%the graph. Due to the nature of a bipartite graph, these
%algorithms have the restriction that the clusters from
%different types of objects must have one-toone associations.
%Clustering on bi-type relational data is called co-clustering
%or bi-clustering. Recently, co-clustering has been addressed
%based on matrix factorization. Both Long et al. (2005) and Li
%(2005) model the co-clustering as an optimization problem
%involving a triple matrix factorization. Long et al. (2005)
%propose an EM-like algorithm based on multiplicative updating
%rules and Li (2005) proposes a hard clustering algorithm for
%binary data. Ding et al. (2005) extend the non-negative matrix
%factorization to symmetric matrices and show that it is
%equvilent to the Kernel Kmeans and the Laplacian-based spectral
%clustering. Several previous efforts related to co-clustering
%are model based. PLSA (Hofmann, 1999) is a method based on a
%mixture decomposition derived from a latent class model. A
%twosided clustering model is proposed for collaborative
%filtering by Hofmann and Puzicha (1999). Information-theory
%based co-clustering has also attracted attention in the
%literature. El-Yaniv and Souroujon (2001) extend the
%information bottleneck (IB) framework (Tishby et al., 1999) to
%repeatedly cluster documents and then words. Dhillon et al.
%(2003) propose a co-clustering algorithm to maximize the mutual
%information between the clustered random variables subject to
%the constraints on the number of row and column clusters. A
%more generalized co-clustering framework is presented by
%Banerjee et al. (2004) wherein any Bregman divergence can be
%used in the objective function. Comparing with co-clustering,
%clustering on general relational data, which may consist of
%more than two types of data objects, has not been well studied
%in the literature. Several noticeable efforts are discussed as
%follows. Taskar et al. (2001) extend the the probabilistic
%relational model to the clustering scenario by introducing
%latent variables into the model. Gao et al. (2005) formulate
%star-structured relational data as a star-structured m-partite
%graph and develop an algorithm based on semi-definite
%programming to partition the graph. Like bipartite graph
%partitioning, it has limitations that the clusters from
%different types of objects must have one-to-one association.
%
%Data co-clustering refers to the problem of simultaneous clustering of two
%data types. Typically, the data is stored in a contingency or co-occurrence matrix C
%where rows and columns of the matrix represent the data types to be co-clustered. An
%entry Cij of the matrix signifies the relation between the data type represented by row
%i and column j . Co-clustering is the problem of deriving sub-matrices from the larger
%data matrix by simultaneously clustering rows and columns of the data matrix. In this
%paper, we present a novel graph theoretic approach to data co-clustering. The two data
%types are modeled as the two sets of vertices of a weighted bipartite graph. We then
%propose Isoperimetric Co-clustering Algorithm (ICA)ï¿½a newmethod for partitioning
%the bipartite graph. ICA requires a simple solution to a sparse system of linear equations
%instead of the eigenvalue or SVD problem in the popular spectral co-clustering
%approach. Our theoretical analysis and extensive experiments performed on publicly
%available datasets demonstrate the advantages of ICA over other approaches in terms
%of the quality, efficiency and stability in partitioning the bipartite graph.
%
%Co-clustering, or simultaneous clustering of rows and columns of a two-dimensional data matrix, is rapidly becoming a powerful data analysis technique. Co-clustering has enjoyed wide success in varied application domains such as text clustering, gene-microarray analysis, natural language processing and image, speech and video analysis. In this paper, we introduce a partitional co-clustering formulation that is driven by the search for a good matrix approximationï¿½every co-clustering is associated with an approximation of the original data matrix and the quality of co-clustering is determined by the approximation error. We allow the approximation error to be measured using a large class of loss functions called Bregman divergences that include squared Euclidean distance and KL-divergence as special cases. In addition, we permit multiple structurally different co-clustering schemes that preserve various linear statistics of the original data matrix. To accomplish the above tasks, we introduce a new minimum Bregman information (MBI) principle that simultaneously generalizes the maximum entropy and standard least squares principles, and leads to a matrix approximation that is optimal among all generalized additive models in a certain natural parameter space. Analysis based on this principle yields an elegant meta algorithm, special cases of which include most previously known alternate minimization based clustering algorithms such as kmeans and co-clustering algorithms such as information theoretic (Dhillon et al., 2003b) and
%minimum sum-squared residue co-clustering (Cho et al., 2004).
%
%For bipartite graph partitioning, spectral approach is the only
%one that has been successfully developed theoretically and
%widely applied for co-clustering. In order to compute these
%partitions, we also need to solve a generalized eigenvalue
%problem as in equation (9). However, due to the bipartite
%nature of the problem, the eigenvalue problem reduces to a much
%efficient Singular Value Decomposition (SVD) [Golub and
%Van-Loan, 1989] problem. [Dhillon, 2001] and [Zha et al., 2001]
%employed this Spectral-SVD approach to partition a bipartite
%graph of documents and words. [Ding, 2003a] performed
%document-word co-clustering by extending Hopfield networks
%[Hopfield, 1982] to partition bipartite graphs and showed that
%the solution is the principal component analysis (PCA)
%[Jolliffe, 2002]. In [Zha and Ji, 2002], the two types of
%vertices of bipartite graph are used to represent sentences of
%documents of two different languages. The Spectral-SVD method
%is applied to identify subgraphs of the weighted bipartite
%graph which can be considered as corresponding to sentences
%that correlate well in textual contents. This algorithm has
%also found application in multimedia co-clustering problems. In
%[Rege et al., 2006b], it has been used to co-cluster a
%bipartite graph of user relevance feedback logs and low-level
%image features. [Wu et al., 2005] have used it on a bipartite
%graph where news stories represent one type of nodes while
%features (textual and visual) extracted from video keyframes
%represent the other. In [Kumar et al., 2004], two algorithms
%have been proposed to extract story lines from user search by
%constructing a word-document bipartite graph. One is a
%heuristic based iterative local search algorithm while the
%other is dynamic programming based algorithm to identify dense
%subgraphs. In [Qiu, 2004], images and their low-level features
%were modeled using a bipartite graph and Hopfield model based
%stochastic algorithm was employed for co-clustering. Bipartite
%graph partitioning for co-clustering have also been applied in
%the field of bioinformatics. In [Ding, 2003b], co-clustering is
%performed on a genes-tissues bipartite graph using the
%Spectral-SVD approach. In [Ding et al., 2004] a bipartite graph
%is used to represent protein and protein complex relationship,
%out of which protein-protein and complex-complex interactions
%arise naturally. It is shown that co-clustering produces
%meaningful protein modules and supercomplexes. In the next
%section, we derive the proposed algorithm for data
%co-clustering using weighted bipartite graphs and show that our
%algorithm requires a simple solution to a sparse system of
%linear equation to partition the bipartite graph.
%
%\section{Transductive Learning}
%\cite{TLBelkin04semisupervisedlearning}, \cite{TLBelkinmanifoldregularization},
%\cite{TLChapelle05semisupervisedclassification}, \cite{TLJoachims03transductivelearning},
%\cite{TLKondor02diffusionkernels}, \cite{TLLafferty04kernelconditional}
%
%
%%\section{Toy Models}
%%\subsection{Gaussian Mixture in 2D interpreted as proximity data}
%%Let's take a look at a 3 class mixture model in 2d - $N_1=(\mu_1,\Sigma_1), N_2 =(\mu_2,\Sigma_2), N_3=(\mu_3,\Sigma_3)$.
%%We can interpret the simulated points as similarity measures of higher dimensional data.  Let $X_n=\{X_1, ... X_n\}$ where $X_i \in R^k$ is our feature data and $\phi(X_i,X_j)$ a non-linear similarity measure producing a dimensionality reduction $R^k \mapsto R^2$.  Possible algorithms for this are PCA, ICA, Spectral Clustering.
%%
%%\begin{figure}
%%  % Requires \usepackage{graphicx}
%%  \includegraphics[width=10cm]{Figures/3class_GaussianMixture_2dim.pdf}\\
%%  \caption{3 Class Guassian Mixture with isoclines of constant probability}
%%\end{figure}
%%
%%
%%\begin{figure}
%%  % Requires \usepackage{graphicx}
%%  \includegraphics[width=10cm]{Figures/Mahalanobis_DistToFirstClass.pdf}\\
%%  \caption{3 Class Guassian Mixture as proximity data to the first class}
%%\end{figure}

%\subsection{Recommender Systems}
%Collaborative filtering, also known as target or personalized
%recommendation systems are software applications that aim to
%support users in their decision-making while interacting with
%high dimensional feature spaces.  Traditional approaches to
%finding items of interest are; content filtering, collaborative
%filtering, and social similarity.
%\begin{itemize}
%  \item Content filtering, or more commonly information
%      retrieval uses features of the items to find similar
%      content.
%  \item Collaborative filtering, uses the preferences or
%      item ranks of other users to recommend items.
%  \item Social filtering, which explicitly uses relations
%      between users to find other items of interest.
%\end{itemize}
%
%Algorithms in practice: LSA, pLDA, PCA, sparse PCA, NNMF,
%Kernel PCA.


\section{Proximity Measurement - central versus pairwise grouping}
Central grouping with K-means of GMM via EM relies on the assumption that feature
vectors for each group have gaussian distribution.  This justifies the use of a Euclidian or Mahalanobis distance metric.

A standard procedure in Machine Learning to to compute a matrix of pairwise similarity measurements that represent proximity of data points.  This is a familiar aspect of multivariate feature data where data dimensions effectively capture class representation.  When presented with graph or network data, the situation becomes a little more complicated.

Propagating pairwise similarity in a transitive fashion avoids the requirement that all members of a cluster are close to some prototype.

Connection subgraph methods were developed for this purpose.

